# Scikit-learn / XGBoost引擎 #

绝大多数算法是基于 [Scikit Learn]() 或者 [XGBoost]() 机器学习库。

引擎提供了内存处理能力，训练集和测试集需要加载到内存中。如有需要可以使用抽样设置。

- [分类算法](#分类算法)
  - [（回归）最小二乘法](#（回归）最小二乘法)
  - [（回归）岭回归](#（回归）岭回归)
  - [（回归）Lasso回归](# （回归）Lasso回归)
  - [（分类）罗辑回归](#（分类）罗辑回归)
  - [（分类与回归）随机森林](#（分类与回归）随机森林)
  - [（分类与回归）梯度提升树](#（分类与回归）梯度提升树)
  - [（分类与回归）XGBoost](#（分类与回归）XGBoost)
  - [（分类与回归）决策树](#（分类与回归）决策树)
  - [（分类与回归）支持向量机](#（分类与回归）支持向量机)
  - [（分类与回归）随机梯度下降](#（分类与回归）随机梯度下降)
  - [（分类与回归）K近邻](#（分类与回归）K近邻)
  - [（分类与回归）极端随机树](#（分类与回归）极端随机树)
  - [（分类与回归）人工神经网络](#（分类与回归）人工神经网络)
  - [（分类与回归）Lasso Path](#（分类与回归）Lasso Path)
  - [（分类与回归）自定义模型](#（分类与回归）自定义模型)
- [聚类算法](#聚类算法)
  - [K均值](#K均值)
  - [混合高斯模型](#混合高斯模型)
  - [Mini-batch K均值](#Mini-batch-K均值)
  - [凝聚层次聚类](#凝聚层次聚类)
  - [谱聚类](#谱聚类)
  - [DBSCAN](#DBSCAN)
  - [交互式聚类（两步聚类）](#交互式聚类（两步聚类）)
  - [孤立森林（异常检测）](#孤立森林（异常检测）)
  - [自定义模型](#自定义模型)

##分类算法##

这种引擎的分类算法支持以下算法。

### （回归）最小二乘法

最小二乘法或者线性最小二乘法是最简单的线性回归算法。目标变量由输入变量加权求和得到。OLS通过cost函数计算合适的权重。

OLS非常简单而且是“可解释”的，但是：

- 它不能自动拟合目标变量，如果目标变量不是输入特征线性组合结果
- 它对输入数据集中的错误高度敏感且容易过拟合

参数：

- **并行度：**并行训练的核数。在训练大量数据集的情况下，用更多的核提高训练速度但会消耗更多内存。(-1代表所有核)

### （回归）岭回归

岭回归通过对权重引入惩罚(正则化项)来解决最小二乘法的一些问题。岭回归使用L2正则化，L2正则化降低模型系数的大小。

参数：

- **正则化项 (自动优化或者确定值)：**自动优化一般比多个确定值速度要快，但是不支持稀疏特征(例如文本哈希) 
- **Alpha：**正则化项，可以指定以逗号分割的多个值列表，这会增加训练时间。

### （回归）Lasso回归


Lasso 回归是另外一种线性回归，它使用不同的正则化项(L1正则化) 。L1正则化降低最终模型的特征数量。

参数：

- **正则化项 (自动优化或者确定值)：**自动优化一般比多个确定值速度要快，但是不支持稀疏特征(例如文本哈希) 
- **Alpha：**正则化项，可以指定以逗号分割的多个值列表，这会增加训练时间。

### （分类）罗辑回归

罗辑回归是采用线性模型的分类算法(通过将输入特征线性合并后计算目标特征)。罗辑回归最小化一个cost 函数(称为logit或者sigmoid函数)，使其更适用于分类。一个简单的逻辑回归算法容易过拟合同时对输入数据集中的错误也很敏感。为了解决这些问题，可以对权重使用惩罚(或者正则项)。

逻辑回归有两个参数：

- **正则化 (L1，L2 正则化)：**L1 正则化减少模型的特征数量。L2正则化降低每个特征的系数。
- **C：**错误项的惩罚参数C。较低的C值会产生一个平滑决策边界(较高方差)，较高的C值目的是将所有的训练数据都正确分类，但是有导致过拟合(较高偏差)。(C对应于正则化参数的逆)。可以用逗号隔开的列表指定多个C值。

### （分类与回归）随机森林

决策树是一种只建立一棵决策树的分类算法。决策树的每个节点包含了某个输入特征中的条件。

回归决策树由多棵决策树组成。在预测过程中，每棵树独立预测，森林中的每棵树进行“投票”。森林对每棵树的结果进行平均。在森林成长(训练)过程中：

- 对于每棵树，随机抽样训练数据集；
- 在树的每个决策点，考虑随机选取输入特征中的一个子集。

参数：

- **树的数量：**森林中树的数量。增加随机森林中树的数量可以避免过拟合。可以用逗号隔开的列表指定多个值，这将会增加训练时长。
- **特征采样策略：**调整每次分裂的特征数量。
  - 自动选择30%的特征
  - 平方根或者以2为基数的对数
  - 特征数量的固定值
  - 特征数量的固定百分比
- **树的最大深度：**森林中每棵树的最大深度。较大的深度通常增加预测的准确度，但可能会导致过拟合。设置为0表示无限深度(树持续分裂指导每个节点只保留一个目标值)
- **每个叶子的最小样本数：**单个节点在分裂时所需要的最小样本数量。较小的值增加预测的准确度(通过分裂树)，但是会导致过拟合切增加训练和预测时间。
- **并行度：**并行训练的核数。在训练大量数据集的情况下，用更多的核提高训练速度但会消耗更多内存。(-1代表所有核)

###（分类与回归）梯度提升树###

梯度提升树是基于决策树的另外一种融合模型。通过一定顺序添加树，每棵树都会用于提升整个融合模型的性能。GBRT的优点在于：

- 天然处理混合数据类型(异构特征)
- 预测能力
- 对输出空间中异常值的鲁棒性(通过鲁棒的损失函数)

请注意有可能会面临扩展性问题，因为顺序提升的特效它很难做到并行。

梯度提升树有四个参数：

- **提升阶段数量：**要执行的提升阶段数量。梯度提升对过拟合鲁棒性很好，因此更大的值将会得到更好的结果。可以用逗号隔开的列表指定多个值，这将会增加训练时长。
- **学习率：**学习率通过learning_rate参数降低每棵树的贡献率。在学习率和提升阶段数量之间需要做个折中。更小的学习率需要更大的提升阶段数量。可以用逗号隔开的列表指定多个值，这将会增加训练时长。
- **Loss：**取决于是分类问题还是回归问题可供选择不同的损失函数。
  - **分类：**对数似然损失函数deviance(相当于逻辑回归)。对于指数损失函数exponential相当于AdaBoost算法。
  - **回归：**可以选择均方差，绝对损失或者Huber。Huber是均方差和绝对损失的结合。
- **树的最大深度：**树融合的最大深度。最大深度限制了树节点的数量。此参数的优化值依赖于输入变量。可以用逗号隔开的列表指定多个值，这将会增加训练时长。

此算法也提供特征的部分依赖可视化图表。

### （分类与回归）XGBoost

XGBoost使用独立的算法库。

XGBoost是一种高级的梯度提升树算法。它支持并行处理、正则化、early stopping，这使得算法处理速度、扩展性和准确度方面更有优势。

参数：

- **树的最大数量：**XGBoost有early stop机制，这可以优化具体树的数量。更大的实际树的数量会增加训练和预测时间。通常设置：100 - 10000
- **Early stopping：**使用XGBoost内置的early stop机制可以优化具体树的数量。在训练和验证Tab中定义的交叉验证将会被用到。
- **Early stopping轮数：**如果连续的N个迭代loss值不会下降，优化器将停止优化。通常值是：1-100
- **树的最大深度：**每棵树的最大深度。较大的值会提高模型的准确度，但是可能会导致过拟合。通常值是：3-10。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **学习率：**较小的值会导致收敛变慢但是会提高模型的鲁棒性。通常值是：0.01 - 0.3。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **L2正则化：**L2正则化会减少各个特征之间的相关系数。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **L1正则化：**为降低过拟合，在高纬数据上可以加速模型评分速度。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Gamma：**指定节点分裂时loss函数的最小值。以指定以逗号分割的多个值列表。这会增加训练时间。
- **子节点最小权重：**节点最小样本权重和。更大的值会避免过拟合，更小的值会让叶子结点匹配最小的数据集，对于高度不均衡的样本集是很有意义的。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Subsample：**子样本比例是每棵树训练的样本数。较低的值可以防止过拟合但是对特殊场景更难学习。通常值是：0.5 - 1。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Colsample by tree：**每棵树使用的特征数量比例。通常值是：0.5 - 1。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **替换缺失值：**是否对缺失值进行替换。
- **并行度：**并行训练时的核数。使用更多核会加速训练也会使用更多的内存，尤其是在大训练数据集的情况下。（-1 表示所有核）

###（分类与回归）决策树###

决策树（DTs）是一个非参数的分类和回归监督学习方法。它的目的是从数据特征中学习出一个简单的决策规则，在这个规则上创建模型来预测变量的目标值。

参数：

- **最大深度：**树的最大深度。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Criterion (Gini or Entropy)：** 用于衡量属性分裂方法的质量。支持的方法有“gini”和“entropy”，“gini“表示Gini 不纯度，”entropy“表示信息增益。该参数只用于分类。
- **叶子的最小样本数：**叶子结点所需的最小样本数。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **分裂策略（最好或随机）：**各节点选择的分裂策略。支持的“最好”策略表示选择最好的分裂，“随机”表示选择最好的随机分裂。

### （分类与回归）支持向量机

SVM是一个非常强大的“黑盒”分类算法。通过使用核函数，他可以学习复杂非线性模型的决策边界（例如，不能在输入特征中学习可以计算目标的线性组合）。SVM对高纬特征很高效，但是通常SVM会比其他算法更慢。

参数：

- **Kernel (linear, RBF, polynomial, sigmoid)：**用于计算样本相似度的核函数。可以尝试多个以确定哪个是最好的。
- **C：**误差项的惩罚参数，较小的C值会产生较平滑的决策边界（较高偏差）。然而较大的值用于正确分类所有的训练样本，这会存在过拟合的风险（较高方差）。（C对应正则化参数）。可以尝试以逗号分割的多个C值列表。
- **Gamma：**当kernel为RBF，polynomial和sigmoid时核函数的系数。Gamma定义每个特征空间中的训练样本的“影响力”。较小的gamma值表示每个样本有广泛的影响，较大的gamma值表示每个样本有较小范围的影响。如果没有指定值（或者设置为0.0），将会设置为1/nb_features。可以尝试以逗号分割的多个Gamma值列表。
- **Tolerance：**训练结束的误差标准。
- **最大迭代次数：**模型训练时的最大迭代数。-1表示没有限制。

### （分类与回归）随机梯度下降##

SGD是重用SVM和逻辑回归原理的算法集。SGD使用一个优化算法最小化成本（或损失）函数，使得它非常适用于大数据集（或者高纬特征）场景。

参数：

- **损失函数（logit或者modified Huber）：**选择“logit”会使得SGD成为一个逻辑回归。使用“modified huber”损失函数会使得SGD成为一个SVM。
- **迭代次数：**训练迭代次数。
- **惩罚项（L1, L2或者elastic net）：**L1和L2正则化与逻辑回归中的类似。Elastic net正则化是L1和L2正则化的合并。
- **Alpha：**正则化参数。较高的alpah值会产生较平滑的决策边界（较高偏差）。然而较小的值用于正确分类所有的训练样本，这会存在过拟合的风险（较高方差）。可以尝试以逗号分割的多个alpha值列表。
- **L1 ratio：**ElasticNet正则化是L1和L2正则化的合并。该比例控制L2正则化的比例（例如：0对应只使用L2，1表示只使用L1）。默认值是0.15（L2占85%，L1占15%）。
- **并行度：**并行训练时的核数。使用更多核会加速训练也会使用更多的内存，尤其是在大训练数据集的情况下。

### （分类与回归）K近邻###

K紧邻分类算法通过找出样本最相近的N个近邻样本然后分配给最合适的分类来对样本进行预测。

*注意：*该算法需要在模型中存储整个数据集，如果训练数据集很大的时候会导致预测变慢。

参数：

- **K：**对每个样本要检查的近邻样本个数。可以用逗号隔开的列表指定多个值，这将会增加训练时长。
- **距离权重：**如果启用后，每个样本的投票权重将会以距离的反比计算。
- **近邻发现方法：**用于找出每个点点最相近邻居的方法。对于预测效果没有影响，但会对训练和预测时间有很大的影响。
  - Automatic：在数据集上通过算法经验选择方法。
  - KD & Ball Tree：将数据存储在多个分区中来加速查找。
  - Brute force：将对每个预测数据计算其与所有训练数据的距离，通常是低效的。
- **p：**Minkowski度量用于查找近邻，当p=2时使用欧式距离，当p=1时使用曼哈顿距离。

###（分类与回归）极端随机树###

极端随机树，和随机森林形似，它也是一个融合模型。除了在树分裂阶段抽样特征以外，它还会在分裂的时候随机选择阈值。这种额外的随机机制将会增强模型的泛化能力。

参数：

- **树的数量：**森林中树的数量。可以用逗号隔开的列表指定多个值，这将会增加训练时长。
- **特征采样策略：**调整每次分裂的特征数量。
  - 自动选择30%的特征
  - 平方根或者以2为基数的对数
  - 特征数量的固定值
  - 特征数量的固定百分比
- **树的最大深度：**森林中每棵树的最大深度。较大的深度通常增加预测的准确度，但可能会导致过拟合。设置为0表示无限深度(树持续分裂指导每个节点只保留一个目标值)
- **每个叶子的最小样本数：**单个节点在分裂时所需要的最小样本数量。较小的值增加预测的准确度(通过分裂树)，但是会导致过拟合切增加训练和预测时间。
- **并行度：**并行训练的核数。在训练大量数据集的情况下，用更多的核提高训练速度但会消耗更多内存。(-1代表所有核)

### （分类与回归）人工神经网络###

受神经元功能的启发，神经网络是一种参数化模型。它由多个“隐藏”神经元层组成，“隐藏”层接收输入并转换参数作为下一层的输入，将分线性方法作用于模型输入，可以实现复杂的分类决策。

参数：

- **隐藏层大小：**各隐藏层神经元数量。指定多个逗号分开的值会增加更多隐藏层。
- **激活函数：**网络中神经元之间的激活函数。
- **Alpha：**L2正则化参数。较大的值会使神经元权重更小泛化能力更强，模型更平滑。
- **最大迭代次数：**最大学习迭代次数。更大的值会让模型收敛更好，但是会需要更长时间。
- **Convergence tolerance：** 如果两个训练迭代的loss提升值小于此比例，训练将会停止。
- **Early stopping：**该模型是否会使用验证early stop机制。
- **Solver：**用于优化的优化器。LBFGS是批量处理算法不适用于大数据集。
- **Shuffle data：**是否对每个epoch之间进行数据shuffle（建议进行shuffle，除非数据已经是随机排序的）。
- **初始学习率：**梯度下降的初始学习率。
- **Automatic batching：** 是否自动生成批量大小（如果数据集较小将会使用200或者整个数据集）。不勾选之后指定批量大小。
- **beta_1：**ADAM优化器的beta_1参数。
- **beta_2：**ADAM优化器的beta_2参数。
- **epsilon：**ADAM优化器的epsilon参数。

###（分类与回归）Lasso Path###

Lasso Path是一种计算LASSO path的算法（所有值的正则化参数），这是用LARS回归实现的。它要求数据量与特征数量一致。如果数据量过大，计算将会很慢。也可以在计算是指定一个非零的相关系数，例如一定数量的特征。训练结束后，可以可视化展示指定数量特征的LASSO path。

参数：

- **最大特征数量：**保留的特征数量，0表示启用所有特征（无正则化）。这对训练时间没有影响。

### （分类与回归）自定义模型###

用户可以使用Python自定义模型。

用户自定义模型需遵循scikit-learn的预测协议，定义适当的`fit`和`predict`方法。

可以参考用户自定义模型样例代码。

## 聚类算法

###K均值###

k-means算法通过将样本分割成n个组来对数据进行聚类，最小化分组对“惯性”。

参数：

- **簇数量：**以指定以逗号分割的多个值列表，这会增加训练时间。
- **Seed：**用来产生可以重复得到的结果。0或者无值表示不适用种子（结果可能不会完全重现）
- **并行度：**用于并行训练的核的数量。用更多的核提高训练速度但会消耗更多内存。-1代表所有核，小于-1的值将会使用（n_cpus + 1 + 该值）个核，例如-2表示使用除去其中一个核以外的所有核。

### 混合高斯模型###

混合高斯模型将将数据拟合到多个“混合的”数据分布上，单个高斯模型可以由一个多变量分布描述。

一个例子是成人的数量分布，它可以由两个分布混合来描述：男人的数量和女人的数量，单个分布可以通过一个正态分布来准确描述。

参数：

- **混合模型数量：**混合高斯模型个数。可以指定以逗号分割的多个值列表，这会增加训练时间。
- **最大迭代次数：**学习模型的最大迭代次数。混合高斯模型采用EM(Expectation-Maximization)算法，EM算法是迭代算法，每次迭代会在整个数据集上运算。因此较大的值会导致训练时间变长，同时聚类精度会增加。建议10到100之间到值。
- **Seed：**用来产生可以重复得到的结果。0或者无值表示不适用种子（结果可能不会完全重现）。

###Mini-batch K均值###

Mini-Batch k-means算法是k-means算法的变种，采用小批量来降低计算时间，同时仍然尝试优化目标函数。

参数：

- **聚类数量：**可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Seed：**用来产生可以重复得到的结果。0或者无值表示不适用种子（结果可能不会完全重现）。

###凝聚层次聚类###

层次聚类是聚类算法的一种，它通过先构建内部聚类然后逐个合并后形成聚类。层次聚类是由树（或[dendrogram](http://en.wikipedia.org/wiki/Dendrogram)来表示。树的根节点表示包含所有样本的唯一聚类，叶子结点表示包含有一个样本的聚类。

参数：

- **聚类数量：**可以指定以逗号分割的多个值列表，这会增加训练时间。

### 谱聚类###

谱聚类通过计算图中近邻结点的距离来实现。它根据相似度矩阵求出低纬拉普拉斯矩阵向量，然后在此低纬空间中用k-means聚类。

参数：

- **聚类数量：**可以指定以逗号分割的多个值列表，这会增加训练时间。
- **Affinity measure：**用来计算 样本相似度距离的方法。可选的方法有：k近邻, RBF高斯核函数和多项式核函数。
- **Gamma：**RBF高斯核函数和多项式核函数的核函数系数。Gamma定义每个特征空间中的训练样本的“影响力”。较小的gamma值表示每个样本有广泛的影响，较大的gamma值表示每个样本有较小范围的影响。如果没有指定值（或者设置为0.0），将会设置为1/nb_features。可以尝试以逗号分割的多个Gamma值列表。
- **Coef0：** 多项式核sigmoid核函数的独立项。
- **Seed：**用来产生可以重复得到的结果。0或者无值表示不适用种子（结果可能不会完全重现）。

###DBSCAN###

DBSCAN算法认为同一聚类的样本有较高的密度。由于DBSCAN这种通俗的思想，DBSCAN得到的聚类可以是任意形状的，不同于k-means的是k-means假设所有的聚类型转都是凸的。对于数值类的特征需要进行标准化。

参数：

- **Epsilon：**把两个样本归于同一聚类的最大距离值。可以尝试以逗号分割的多个值列表。
- **Min. Sample ratio：** 聚类中样本的最小比例。

###交互式聚类（两步聚类）###

交互式聚类基于一种两步聚类算法。这种两阶段聚类算法首先用K-Means将数据聚成小簇。然后应用凝聚层次聚类进一步聚类数据，同时在小簇之间构建层次结构，然后去解释。因此它运行从大于几百行的数据集中提取层次信息，这是通过标准方法做不到的。在DSS用户界面中可以收到调整聚类的。

参数：

- **预聚类数量：**K-Means预先聚类的数量。为保证可读性建议设置小于几百的值。
- **聚类数量：**层次聚类的数量。将会构建和显示完全的层次结构，这些聚类用于评分。
- **最大迭代次数：**预聚类的最大迭代次数。K-Menas是迭代算法，每次迭代会在整个数据集上运算。因此较大的值会导致训练时间变长，同时聚类精度会增加。建议10到100之间到值
- **Seed：**用来产生可以重复得到的结果。0或者无值表示不适用种子（结果可能不会完全重现）。

###孤立森林（异常检测）###

Isolation forest是一种异常检测算法。它通过创建随机森林，森林中的每棵树将样本分裂到不同的分区中。异常点通常距离根节点有更短的路径。因此，距离根的平均距离提供了一个良好的非正态度量。

参数：

- **树的数量：**森林中树的数量。
- **Contamination：**数据中预期的异常比例。
- **Anomalies to display：**模型报告中显示的最大异常样本数量。过大的值可能导致内存和UI问题。

### 自定义模型###

用户可以使用Python自定义模型。

用户自定义模型需遵循scikit-learn的预测协议，定义适当的`fit`和`fit_predict`方法。

模型接口也可以传递具体的聚类数量。

可以参考用户自定义模型样例代码。